{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression Modeling with Boston Housing dataset - Lab\n",
    "\n",
    "In this final lab, we shall apply the regression analysis and diagnostics techniques covered in this section to a familiar \"Boston Housing\" dataset. We performed a detailed EDA for this dataset in earlier section and hence carry a good understanding of how this dataset is composed. This this lab we shall try to identify the predictive ability of some of features found in this dataset towards identifying house price. \n",
    "\n",
    "### Objectives:\n",
    "You will be able to:\n",
    "* Build many linear models with boston housing data set using OLS\n",
    "* For each model, analyze OLS diagnostics for model validity \n",
    "* Visually explain the results and interpret the diagnostics from Statsmodels \n",
    "* Comment on the goodness of fit for a simple regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and load 'BostonHousing.csv' as pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "plt.style.use('ggplot')\n",
    "boston = pd.read_csv('BostonHousing.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data features and target are present as columns in boston data. Boston data gives a set of independent as independent variables in data and the housing rate as `MEDV` in target property. Also feature names are listed in feature_names. The desription is available at [KAGGLE](https://www.kaggle.com/c/boston-housing). \n",
    "\n",
    "### Inspect the columns of the dataset and comment on type of variables present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(boston.info())\n",
    "# len(boston.crim.unique().tolist()), len(boston.zn.unique().tolist()),len(boston.indus.unique().tolist()),\n",
    "# len(boston.chas.unique().tolist()),len(boston.nox.unique().tolist()), len(boston.rm.unique().tolist()),\n",
    "for column in ['rad', 'chas','tax']:\n",
    "    print(column, 'len unique values =',len(boston[column].unique().tolist()))\n",
    "    print(column, ' unique values:', boston[column].unique().tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your observations here \n",
    "# there are 506 rows, 14 columns and no null and missing values\n",
    "# The dataset mostly contains continuous variables\n",
    "# chas is categorical - (Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)) \n",
    "# rad-index of accessibility to radial highways,  is also categorical, with 9 unique values\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create histograms for all variables in the dataset and comment on their shape (uniform or not ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "boston.hist(figsize=(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for skewness & kurtosis\n",
    "for column in boston: \n",
    "    print ('Skewness =',column,  skew(boston[column]))\n",
    "    print ('kurtosis =',column, kurtosis(boston[column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You observations here \n",
    "# indus and rm are fairly symmetrical \n",
    "# positively skewed: crim, zn, rad, dis, medv\n",
    "# negatively skewed: b\n",
    "# Leptokurtic: crim, zn, chas, b\n",
    "# platykurtic:  indus, nox, rm, age, dis, rad, tax, ptratio, lstat, medv\n",
    "# positively skewed: crim, zn, rad, dis, medv\n",
    "# negatively skewed: b\n",
    "# Some variables have outliers at extreme tails (b, zn, crim, rm)\n",
    "# the target variable looks good with some outliers in the right tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this , we shall choose a selection of features which appear to be more 'normal' than others.\n",
    "### Create a new dataset with `['crim', 'dis', 'rm', 'zn', 'age', 'medv']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_new = boston[['crim', 'dis', 'rm', 'zn', 'age', 'medv']].copy()\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for linearity assumption for all chosen features with target variable using scatter plots and comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "for column in ['crim', 'dis', 'rm', 'zn', 'age']:\n",
    "    plt.scatter(data[column], data.medv, label=column)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "for column in df_new:\n",
    "    plt.scatter(df_new[column], df_new.medv, label=column)\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations here \n",
    "# crim variable's linearity seemd a bit unclear as the values are too close to each other and generally very small\n",
    "# there is SOME linearity apparent in variables although the variance along y-axis is a bit unpredictable for some values\n",
    "# Some outliers present in almost all cases\n",
    "# Data probably needs more normalization and pre-processing to \"Clean it up\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okie so obviously our data needs a lot of pre-procesing to improve the results. This key behind such kaggle competitions is to process the data in such a way that we can identify the relationships and make predictions in the best possible way. For now, we shall leave the dataset untouched and just move on with regression. So far, our assumptions, although not too strong, but still hold to a level that we can move on. \n",
    "\n",
    "### Let's do Regression \n",
    "\n",
    "Right here is the real deal. Let's perform a number of simple regression experiments between the chosen independent variables and the dependent variable (price). We shall do this is a loop and in every iteration, we shall pick one of the independent variables  perform following steps:\n",
    "\n",
    "* Run a simple OLS regression between independent and dependent variables\n",
    "* Plot a regression line on the scatter plots\n",
    "* Plot the residuals using `sm.graphics.plot_regress_exog()`.\n",
    "* Plot a Q-Q plot for regression residuals normality test \n",
    "* Store following values in array for each iteration:\n",
    "    * Independent Variable\n",
    "    * r_squared'\n",
    "    * intercept'\n",
    "    * 'slope'\n",
    "    * 'p-value'\n",
    "    * 'normality (JB)' \n",
    "* Comment on each output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "\n",
    "\n",
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value', 'normality (JB)' ]]\n",
    "for idx, val in enumerate(['crim', 'dis', 'rm', 'zn', 'age']):\n",
    "    print (\"Boston Housing DataSet - Regression Analysis and Diagnostics for formula: medv~\" + val)\n",
    "    print (\"-------------------------------------------------------------------------------------\")\n",
    "\n",
    "    f = 'medv~' + val\n",
    "#   \n",
    "    model = smf.ols(formula=f, data=data).fit()\n",
    "    \n",
    "    X_new = pd.DataFrame({val: [data[val].min(), data[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    data.plot(kind='scatter', x=val, y='medv');\n",
    "    plt.plot(X_new, preds, c='red', linewidth=2);\n",
    "    plt.show()\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    fig = sm.graphics.plot_regress_exog(model, val, fig=fig)\n",
    "    fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True,   )\n",
    "    plt.show()\n",
    "    \n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1], sms.jarque_bera(model.resid)[0] ])\n",
    "    input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your obervations here \n",
    "# We can do a detailed analysis of each exsperiment and elaborate in detail \n",
    "# Here we shall show a summary of selected observations\n",
    "\n",
    "# Crime has a negative relationship with price i.e. less crime > higher price and vice vera\n",
    "# Crime does not show any clear signs heteroscedasticity \n",
    "# Crime has a low r-squared so not such a good fit \n",
    "# Residuals not normally distributed (needs log normalization that we'll see in next section)\n",
    "\n",
    "# a positive relationship between dis and medv\n",
    "# dis residual plots show some signs of heteroscadasticity as cone shaped residuals\n",
    "# normality is still questionable \n",
    "\n",
    "# rm shows a strong positive relationship\n",
    "# rm residuals show no signs of heteroscdasticity however some outliers are present\n",
    "# rm qqplot shows a long right tail which hurts normality \n",
    "\n",
    "# zn variable scatter shows a lot of varianc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So clearly the results are not highly reliable. the best good of fit i.e. r-squared is witnessed with `rm`. So clearly in this analysis this is our best predictor. \n",
    "\n",
    "---\n",
    "#### So how can we improve upon these results . \n",
    "1. Pre-Processing \n",
    "\n",
    "This is where pre-processing of data comes in. Dealing with outliers, normalizing data, scaling values etc can help regression analysis get more meaningful results from the given set of data \n",
    "\n",
    "2. Advanced Analytical Methods\n",
    "\n",
    "Simple regression is a very basic analysis techniques and trying to fit a straight line solution to complex analytical questions may prove to be very inefficient. In the next section we shall look at multiple regression where we can use multiple features **AT ONCE** to define a relationship with outcome. We shall also look at some pre-processing and data simplification techniques and re-visit the boston dataset with an improved toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up - Optional \n",
    "\n",
    "Apply some data wrangling skills that you have learned in previous section to pre-process the set of independent variables we chose above. You can start off with outliers and think of a way to deal with them. See how it affects the the goodness of fit. \n",
    "![](https://i.pinimg.com/originals/e5/a5/1e/e5a51eff1b2133105ebaa9b779106ae2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lab, we attempted to bring in all the skills learnt so far to a slighlt detailed dataset. We looked at the outcome of our analysis and realized that the data might need some pre-processing to see a clear improvement in results. We shall pick it up in the next section from this point and bring in data pre-processing techniques along with some assumptions that are needed for multiple regression . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
